%%
%% Class homework & solution template for latex
%% Alex Ihler
%%
\documentclass[twoside,11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx,color}
\usepackage{verbatim,url}
\usepackage{listings}
\usepackage{subcaption} 
\usepackage{upquote}
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage[scaled]{beramono}
%\usepackage{textcomp}

% Directories for other source files and images
\newcommand{\bibtexdir}{../bib}
\newcommand{\figdir}{fig}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\matlab}{{\sc Matlab}\ }

\setlength{\textheight}{9in} \setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{-.25in}  % Centers text.
\setlength{\evensidemargin}{-.25in} %
\setlength{\topmargin}{0in} %
\setlength{\headheight}{0in} %
\setlength{\headsep}{0in} %

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

\theoremstyle{definition}
\newtheorem{MatEx}{M{\scriptsize{ATLAB}} Usage Example}

\definecolor{comments}{rgb}{0,.5,0}
\definecolor{backgnd}{rgb}{.95,.95,.95}
\definecolor{string}{rgb}{.2,.2,.2}
\lstset{language=Matlab}
\lstset{basicstyle=\small\ttfamily,
        mathescape=true,
        emptylines=1, showlines=true,
        backgroundcolor=\color{backgnd},
        commentstyle=\color{comments}\ttfamily, %\rmfamily,
        stringstyle=\color{string}\ttfamily,
        keywordstyle=\ttfamily, %\normalfont,
        showstringspaces=false}
\newcommand{\matp}{\mathbf{\gg}}




\begin{document}

\centerline{\Large Homework 2}
\centerline{Zachary DeStefano, 15247592}
\centerline{CS 274B: Spring 2016}

\section*{Problem 1: }

\subsection*{Part A,B}

Here is the code I used to produce the mutual information
\begin{lstlisting}
#Part A
D = np.genfromtxt('data/data.txt',delimiter=None)
loc = np.genfromtxt('data/locations.txt',delimiter=None)
m,n = D.shape # m = 2760 data points, n=30 dimensional

#Part B

#Find phat for each variable
posXcount = np.sum(D,axis=0)
probXj = np.zeros((n,2))
probXj[:, 1] = np.divide(posXcount,m)
probXj[:, 0] = 1-probXj[:,1]

#Find phat(x_j,x_k) for each pair of variables
probXjk = np.zeros((n,n,2,2))
for i in range(n):
    for j in range(n):
        for k in range(m):
            probXjk[i, j, D[k, i], D[k, j]] += 1
probXjk = np.divide(probXjk,m)

#Compute mutual information
mutualInfo = np.zeros((n,n))
for i in range(n):
    for j in range(n):
        curTerm = 0
        for k1 in range(2):
            for k2 in range(2):
                denomTerm = probXj[i,k1]*probXj[j,k2]
                jointTerm = probXjk[i, j, k1, k2]
                if denomTerm>0 and jointTerm>0:
                    curTerm += jointTerm*np.log(jointTerm/denomTerm)
        mutualInfo[i, j] = curTerm

#print results
print mutualInfo[0:5,0:5]
\end{lstlisting}

Here are the first 5 rows and columns of mutual information
\begin{lstlisting}
[[ 0.60870002  0.24584358  0.30645904  0.17521613  0.24642811]
 [ 0.24584358  0.66081843  0.28479996  0.21222061  0.29414659]
 [ 0.30645904  0.28479996  0.64609051  0.21486084  0.27873285]
 [ 0.17521613  0.21222061  0.21486084  0.68239551  0.23489376]
 [ 0.24642811  0.29414659  0.27873285  0.23489376  0.67286449]]
\end{lstlisting}
\newpage
\subsection*{Part C}

For the algorithm, I added edges in descending order by weight if they did not make a cycle. To compute this, I first initialized the edge information arrays:
\begin{lstlisting}
edges = np.zeros(n*(n-1)/2)
edgeNodeID = np.zeros((n*(n-1)/2,2))
edgeInd = 0
for i in range(n):
    for j in range(i+1,n):
        edges[edgeInd] = mutualInfo[i, j]
        edgeNodeID[edgeInd,:] = [i, j]
        edgeInd+=1

sortedEdges = np.sort(edges)[::-1]
sortedEdgeID = np.argsort(edges)[::-1]
\end{lstlisting}

I made a function \textit{nodesHavePath} that tells whether an edge will form a cycle
\begin{lstlisting}
def nodesHavePath(adjMatrix,node0,node1):

    n,m = adjMatrix.shape
    adjMatrix = adjMatrix + np.transpose(adjMatrix)
    adjPower = np.copy(adjMatrix)
    adjTotal = np.copy(adjMatrix)
    for i in range(n):
        adjPower = np.dot(adjMatrix, adjPower)
        adjTotal += adjPower

    return (adjTotal[node0,node1]>0)
\end{lstlisting}

I then computed the adjacency matrix for the Chow-Liu Tree:
\begin{lstlisting}
adjMatrix = np.zeros((n,n))
for curI in range(len(sortedEdges)):
    curEdgeID = sortedEdgeID[curI]
    curNode0 = edgeNodeID[curEdgeID, 0]
    curNode1 = edgeNodeID[curEdgeID, 1]
    if not nodesHavePath(adjMatrix,curNode0,curNode1):
        adjMatrix[curNode0,curNode1]=1
\end{lstlisting}

\newpage

To print the edges, I made an adjacency list that does not repeat edges. Here is the code that makes the list and prints out the results
\begin{lstlisting}
def getAdjList(adjMatrix):
    mm,xx = adjMatrix.shape
    visited = np.zeros((mm))
    adjList = []
    listVertices = []
    for i in range(mm):
        visited[i] = 1
        adjVertices = np.where(adjMatrix[i,:]>0)
        verticesAdd = []
        for j in adjVertices[0]:
            if visited[j] <= 0:
                verticesAdd.append(j)
        if len(verticesAdd) > 0:
            adjList.append(verticesAdd)
            listVertices.append(i)
    return listVertices,adjList

listVertices,adjList = getAdjList(adjMatrix)
for i in range(len(listVertices)):
    print listVertices[i],adjList[i]
\end{lstlisting}

Here is the printed result
\begin{lstlisting}
0 [2]
1 [4]
2 [16, 17]
3 [5, 12, 29]
4 [6]
5 [6]
7 [10, 13]
8 [13]
9 [10]
10 [14]
11 [12, 14]
13 [15]
17 [18, 20]
18 [19]
20 [27]
21 [24, 25, 27]
22 [23, 26, 28]
27 [29]
28 [29]
\end{lstlisting}
\newpage
Here is the code I used to produce the graph drawing of the Chow-Liu Tree
\begin{lstlisting}
graph2 = nx.Graph()
graph2.add_nodes_from(range(n))
for i in range(len(listVertices)):
		for j in adjList[i]:
				graph2.add_edge(listVertices[i],j)
loc2 = np.zeros(loc.shape)
loc2[:, 0] = loc[:, 1]
loc2[:, 1] = loc[:, 0]
nx.draw_networkx(graph2, node_color='c', pos=loc2)
plt.title('Weather Station Locations with Chow-Liu Tree')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()
\end{lstlisting}

\begin{figure}[h]
\centering
\includegraphics[width=6in]{chowLiuTreeFigure2.png}
\caption{Chow-Liu Tree as computed in Part C}
\end{figure}

\subsection*{Part D}

To calculate the likelihood, I used the method in Slide 6 of \textit{Structure Learning in Bayes Nets} where you add together the entropy and mutual information. Here is the code:
\begin{lstlisting}

#look at diagnoal of mutual info matrix for entropy
mutualInfoEntropy = np.zeros(n)
for i in range(n):
    mutualInfoEntropy[i] = mutualInfo[i,i]
entropyPart = mutualInfoEntropy.sum()

mutualInfoPart = 0
for jj in range(len(listVertices)):
    curJind = listVertices[jj]
    for kk in adjList[jj]:
        mutualInfoPart += mutualInfo[curJind,kk]

loglike = mutualInfoPart - entropyPart

print 'Log Likelihood:'
print loglike
\end{lstlisting}

Here is the result
\begin{lstlisting}
Log Likelihood:
-11.0986143278
\end{lstlisting}

\newpage

\section*{Problem 2:}

\subsection*{Part A}

Here is the loopy model compared with the Chow-Liu Tree Model. The loopy model seems more logical because it resembles a real-world network slightly better. 

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=3 in]{loopyModelFigure2.png}
  \caption{Loopy Model}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=3 in]{chowLiuTreeFigure2.png}
  \caption{Chow-Liu Tree Model}
\end{subfigure}
\caption{Loopy Model and Chow-Liu Tree Model side-by-side}
\end{figure}

Here is the code I used to generate the loopy model graph

\begin{lstlisting}
edges = np.genfromtxt('data/edges.txt')
loc = np.genfromtxt('data/locations.txt',delimiter=None)

graph2 = nx.Graph()
graph2.add_nodes_from(range(n))
for edgeI in edges:
    graph2.add_edge(edgeI[0], edgeI[1])
loc2 = np.zeros(loc.shape)
loc2[:,0] = loc[:,1]
loc2[:,1] = loc[:,0]
nx.draw_networkx(graph2, node_color='c', pos=loc2)
plt.title('Weather Station Locations with Loopy Model')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()
\end{lstlisting}

\newpage

\subsection*{Part B}

Here is the code for the empirical marginal probabilities
\begin{lstlisting}

probXjk = np.zeros((n,n,2,2))
for i in range(n):
    for j in range(n):
        for k in range(m):
            probXjk[i, j, D[k, i], D[k, j]] += 1
probXjk = np.divide(probXjk,m)

edgeMarginals = np.zeros((nEdges,2,2))
for ee in range(nEdges):
    node0 = edges[ee,0]
    node1 = edges[ee,1]
    edgeMarginals[ee,:,:] = probXjk[node0,node1,:,:]
print edgeMarginals[0:5]

\end{lstlisting}

Here are the first 5 empirical marginal probabilities

\begin{lstlisting}
[[[ 0.59130435  0.11123188]
  [ 0.03514493  0.26231884]]

 [[ 0.62391304  0.07862319]
  [ 0.02826087  0.2692029 ]]

 [[ 0.57318841  0.12934783]
  [ 0.02717391  0.27028986]]

 [[ 0.66014493  0.0423913 ]
  [ 0.07427536  0.22318841]]

 [[ 0.65543478  0.04710145]
  [ 0.09637681  0.20108696]]]
\end{lstlisting}
\newpage
\subsection*{Part C}
Here is the code I used to run IPF:
\begin{lstlisting}
edges = np.genfromtxt('data/edges.txt')
nEdges,cc = edges.shape

gmNodes = [gm.Var(i,2) for i in range(nEdges)]
probNodes = [gm.Var(i,2) for i in range(nEdges)]
gmFactors = []
for ee in range(nEdges):
    jj = int(edges[ee,0])
    kk = int(edges[ee,1])
    gmFactors.append(gm.Factor([gmNodes[jj], gmNodes[kk]], 1.0))
    inputFactor = np.matrix(np.ones((2, 2)))
    gmFactors[ee].table = inputFactor

sumElim = lambda F,Xlist: F.sum(Xlist)   # helper function for eliminate

numIter=15
logLikeIter = np.zeros(numIter)
for iterI in range(numIter):
    for ee in range(nEdges):
        jj = int(edges[ee, 0])
        kk = int(edges[ee, 1])

        currentFactors = copy.deepcopy(gmFactors)
        curModel = gm.GraphModel(currentFactors)
        pri = [1.0 for Xi in currentFactors]
        pri[jj], pri[kk] = 2.0, 2.0
        order = gm.eliminationOrder(curModel,orderMethod='minfill',priority=pri)[0]
        curModel.eliminate(order[:-2], sumElim)  # eliminate all but last two
        curP = curModel.joint()
        curLnZ = np.log(curP.sum())
        curP /= curP.sum()

        newFij = gmFactors[ee].table*probXjk[jj,kk,:,:]/curP.table
        gmFactors[ee].table = newFij

    curLog = 0
    probModel = gm.GraphModel(gmFactors)
    for ptNum in range(m):
        curLog += probModel.logValue(D[ptNum, :])#-curLnZ
    curLog = curLog / m - curLnZ
    logLikeIter[iterI] = curLog
    print 'lnZ: ', curLnZ

plt.plot(range(1,numIter+1),logLikeIter)
plt.xlabel('Number of Completed Iterations')
plt.ylabel('Log Likelihood Of Model')
plt.show()

print logLikeIter
\end{lstlisting}
\newpage

Here is the graph of log likelihood vs iteration number:

\begin{figure}[h]
\centering
\includegraphics[width=6in]{prob2cPlot2.png}
\caption{Log likelihood after running IPF}
\end{figure}

The log partition function stayed constant with the following value:\\
$20.7944154168$\\
\\
These were the printed log likelihood values: \\
\begin{lstlisting}
-10.52755241
-9.76622834
-9.70930548
-9.69557976
-9.69224636
-9.6915535
-9.69141484
-9.69138601
-9.69137972
-9.6913783
-9.69137794
-9.69137784
-9.69137781
-9.6913778
-9.6913778
\end{lstlisting}
Thus it converged to a value of $-9.6913778$

\end{document}
